# PhilogicAI Integration

Sichere Verbindung zwischen Production Website und lokaler AI-Inference.

## ğŸ¯ Was wurde gebaut?

- **Chat UI Component** (`app/components/PhilogicAIChat.tsx`)
  - Floating Chat Button (unten rechts)
  - Expandable Chat Window
  - Message History
  - Loading States
  
- **API Proxy Route** (`app/api/philogic-ai/chat/route.ts`)
  - Proxied Requests zu deiner lokalen AI
  - Bearer Token Authentication
  - Error Handling
  
- **Lokaler AI Server** (`philogic-ai-server.py`)
  - Flask Server auf localhost:8000
  - llama.cpp Integration
  - Conversation Context Management
  - Auth Token Protection
  
- **Cloudflare Tunnel Setup** (`setup-cloudflare-tunnel.ps1`)
  - Automatisches Setup Script
  - Sicherer Tunnel von Production zu deinem PC
  - DNS Konfiguration

## ğŸš€ Schnellstart

### Option 1: Automatisches Setup

```powershell
# 1. Cloudflare Tunnel einrichten
.\setup-cloudflare-tunnel.ps1

# 2. Server starten
.\start-philogic-ai.bat

# 3. Next.js starten
npm run dev
```

### Option 2: Manuelles Setup

Siehe `QUICKSTART.md` fÃ¼r detaillierte Schritt-fÃ¼r-Schritt Anleitung.

## ğŸ“ Neue Dateien

| Datei | Beschreibung |
|-------|--------------|
| `app/components/PhilogicAIChat.tsx` | React Chat Component |
| `app/api/philogic-ai/chat/route.ts` | Next.js API Route |
| `philogic-ai-server.py` | Flask Server (zu kopieren nach C:\philogic-ai\) |
| `setup-cloudflare-tunnel.ps1` | Automatisches Cloudflare Setup |
| `start-philogic-ai.bat` | Server Startup Script |
| `PHILOGIC_AI_SETUP.md` | AusfÃ¼hrliche Dokumentation |
| `QUICKSTART.md` | Schnellstart Anleitung |

## âš™ï¸ Konfiguration

### Lokale Entwicklung (.env.local)

```env
PHILOGIC_AI_URL=http://localhost:8000/api/chat
PHILOGIC_AUTH_TOKEN=your-secure-token-here
```

### Production (Cloudflare Pages)

```env
PHILOGIC_AI_URL=https://ai.philogichub.com/api/chat
PHILOGIC_AUTH_TOKEN=your-secure-token-here
```

**Wichtig:** Token muss identisch sein in:
1. `.env.local`
2. `C:\philogic-ai\server.py`
3. Cloudflare Pages Environment Variables

## ğŸ”’ Sicherheit

âœ… Bearer Token Authentication
âœ… Localhost-only Server
âœ… Cloudflare Tunnel (keine offenen Ports)
âœ… HTTPS End-to-End VerschlÃ¼sselung
âœ… Optional: IP Whitelist via Cloudflare WAF
âœ… Optional: Email Auth via Cloudflare Zero Trust

## ğŸ“Š Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Production Flow                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Browser
    â†“
philogichub.com (Cloudflare Pages)
    â†“
/api/philogic-ai/chat (Next.js API Route)
    â†“ HTTPS + Bearer Token
ai.philogichub.com (Cloudflare Tunnel)
    â†“ Localhost
127.0.0.1:8000 (Flask Server auf deinem PC)
    â†“
llama.cpp (Lokale GPU/CPU Inference)
    â†“
Qwen3-14B Model
```

## ğŸ® Model Einstellungen

Im `philogic-ai-server.py` anpassbar:

```python
MAX_TOKENS = 512        # Antwort-LÃ¤nge
THREADS = 8             # CPU Threads
TEMPERATURE = 0.7       # KreativitÃ¤t (0.0-1.0)
TOP_P = 0.9            # Sampling
GPU_LAYERS = 32         # GPU VRAM Nutzung
```

## ğŸ› Troubleshooting

### Chat lÃ¤dt nicht

1. PrÃ¼fe Dev Server Logs (Terminal)
2. Ã–ffne Browser Console (F12)
3. Schaue nach Network Errors

### "PhilogicAI ist nicht verfÃ¼gbar"

1. Server lÃ¤uft? â†’ `http://localhost:8000/health` Ã¶ffnen
2. Token korrekt? â†’ Vergleiche alle 3 Stellen
3. Tunnel lÃ¤uft? â†’ Terminal 2 prÃ¼fen

### "Unauthorized" Error

Auth Token ist nicht identisch oder fehlt:
- In `.env.local`
- In `server.py` (Zeile 22)
- In Cloudflare Pages Environment Variables

## ğŸ“š Weitere Dokumentation

- **QUICKSTART.md** - Schnellstart Guide (5 Schritte)
- **PHILOGIC_AI_SETUP.md** - AusfÃ¼hrliche Setup Dokumentation
- **llama.cpp Docs** - https://github.com/ggerganov/llama.cpp

## âœ… Deployment Checkliste

### Lokal
- [ ] Flask & flask-cors installiert
- [ ] server.py in C:\philogic-ai\ kopiert
- [ ] Pfade in server.py angepasst
- [ ] Auth Token generiert und gesetzt
- [ ] Server startet ohne Fehler
- [ ] Health Check funktioniert (localhost:8000/health)
- [ ] Chat im Browser funktioniert

### Production
- [ ] Cloudflare Tunnel Setup durchgefÃ¼hrt
- [ ] Tunnel lÃ¤uft und ist connected
- [ ] DNS fÃ¼r ai.philogichub.com zeigt auf Tunnel
- [ ] Environment Variables in Cloudflare Pages gesetzt
- [ ] Production Build deployed
- [ ] Chat auf Production Website funktioniert

## ğŸ†˜ Support Kontakte

- **llama.cpp Issues:** https://github.com/ggerganov/llama.cpp/issues
- **Cloudflare Tunnel Docs:** https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/
- **Next.js Docs:** https://nextjs.org/docs

## ğŸ“ˆ Performance Tipps

1. **GPU nutzen:** Stelle sicher dass `GPU_LAYERS > 0` in server.py
2. **Mehr VRAM:** ErhÃ¶he `GPU_LAYERS` bis max. Model Layers
3. **Weniger Tokens:** Reduziere `MAX_TOKENS` fÃ¼r schnellere Antworten
4. **Kleineres Model:** Nutze Q4 statt Q5 quantization
5. **Mehr Threads:** ErhÃ¶he `THREADS` auf CPU Core Count

## ğŸ“ Changelog

### Version 1.0 (15.11.2025)
- âœ… Initial PhilogicAI Integration
- âœ… Chat UI Component mit floating button
- âœ… API Proxy Route mit Authentication
- âœ… Flask Server mit llama.cpp
- âœ… Cloudflare Tunnel Setup Script
- âœ… VollstÃ¤ndige Dokumentation

---

**Status:** âœ… Production Ready

Deine lokale AI lÃ¤uft jetzt sicher auf deinem PC und ist Ã¼ber Cloudflare Tunnel fÃ¼r die Production Website erreichbar!
